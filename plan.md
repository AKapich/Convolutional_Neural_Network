We will be taking inspiration from VGG16 and GoogLeNet architectures, since they are work best for CINIC10 dataset and are ralatively uncomplicated. We also will examin the following things:
- hyperparameter tuning: adjusting parameters such as learning rate, batch size and two dropout rate, weight decay.
- use three augmentation techniques rotation, flipping, scaling and optionally one advanced technique.
- implement a dedicated method for few-shot learning. Reduce the training set size and compare results with those from the full dataset.
- apply at least one ensemble strategy and assess performance improvements.

The timeline is going to be:
1. week: learn about CNN networks and techniques used to enchance their performance
2. week: research exact architectures and implement them
3. week: test different parameters
4. week: implement a method for few-shot learning and ensemble strategy. Next, test those methods
5. week: write a report
